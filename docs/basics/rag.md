---
sidebar_position: 1
slug: /what-is-rag
---

# 什么是检索增强生成（RAG）？

自从大语言模型（LLM）成为技术焦点以来，它们处理通用知识的能力令人惊叹。然而，当问题转向企业内部文档、专有知识库或实时数据时，LLM的局限性变得显而易见：它们无法访问训练数据之外的私有信息。检索增强生成（RAG）正是为解决这一核心需求而诞生。在LLM生成答案之前，它首先从外部知识库中检索最相关的上下文，并将其作为"参考资料"输入给LLM，从而引导它产生准确的答案。简而言之，RAG将LLM从"依赖记忆"提升到"有证据可依"，显著提高了它们在专业领域和实时信息查询中的准确性和可信度。

## 为什么RAG很重要？

尽管LLM在语言理解和生成方面表现出色，但它们存在固有的局限性：

- **静态知识**：模型的知识基于训练时的数据快照，无法自动更新，难以感知最新信息。
- **外部数据盲点**：它们无法直接访问企业私有文档、实时信息流或特定领域的内容。
- **幻觉风险**：当缺乏准确证据时，它们可能仍然会编造听起来合理但错误的答案以维持对话流畅性。

RAG的引入为LLM提供了实时、可信的"事实基础"。其核心机制分为两个阶段：

- **检索阶段**：基于用户的问题，从外部知识库中快速检索最相关的文档或数据片段。
- **生成阶段**：LLM通过将检索到的信息作为上下文结合，利用自身的语言能力组织和生成最终答案。

这将LLM从"凭记忆说话"升级为"有文档支持地说话"，大大增强了在专业和企业级应用中的可靠性。

## RAG如何工作？

检索增强生成通过引入信息检索机制，使LLM能够利用实时、外部或私有数据源生成更高质量的响应。其工作流程可分为以下关键步骤：

### 数据处理和向量化

RAG所需的知识来自各种格式的非结构化数据，如文档、数据库记录或API返回内容。这些数据通常需要被分块，然后通过嵌入模型转换为向量，并存储在向量数据库中。

**为什么需要分块？** 直接索引整个文档面临以下问题：

- **检索精度降低**：向量化长文档会导致语义"平均化"，丢失细节。
- **上下文长度限制**：LLM的上下文窗口有限，需要筛选最相关的部分输入。
- **成本和效率**：长文本的嵌入计算和检索成本更高。

因此，智能的分块策略是平衡信息完整性、检索粒度和计算效率的关键。

### 检索相关信息

用户的查询也被转换为向量，以在向量数据库中执行语义相关性搜索（例如计算余弦相似度），匹配和召回最相关的文本片段。

### 上下文构建和答案生成

检索到的相关内容被添加到LLM的上下文中作为事实基础，LLM最终生成答案。因此，RAG可以被视为自动化上下文构建的上下文工程1.0。

## 深入现有RAG架构：超越向量检索

工业级RAG系统远非"向量搜索+LLM"那么简单；其复杂性和挑战主要体现在检索过程中。

### 数据复杂性：多模态文档处理

**核心挑战**：企业知识主要以包含文本、图表、表格和公式的多模态文档形式存在。简单的OCR提取会丢失大量语义信息。

**高级实践**：领先的解决方案（如RAGFlow）倾向于使用视觉语言模型（VLM）或专门的解析模型（如DeepDoc）将多模态文档"翻译"为富含结构和语义信息的单模态文本。将多模态信息转换为高质量的单模态文本已成为高级RAG的标准实践。

### 分块的复杂性：精度与上下文之间的权衡

简单的"分块-嵌入-检索"流水线存在内在矛盾：
- **语义匹配**需要小的文本块以确保清晰的语义焦点。
- **上下文理解**需要大的文本块以确保完整连贯的信息。

这迫使系统设计在"精确但碎片化"和"完整但模糊"之间做出艰难权衡。

**高级实践**：领先的解决方案（如RAGFlow）采用语义增强技术，如构建语义目录和知识图谱。这些不仅解决了物理分块导致的语义碎片化问题，还使基于实体关系网络发现跨文档的相关内容成为可能。

### 为什么向量数据库不足以服务RAG？

向量数据库擅长语义相似度搜索，但RAG需要精确可靠的答案，对检索系统提出更多能力要求：
- **混合搜索**：仅依赖向量检索可能错过精确的关键词匹配（如产品代码、法规编号）。混合搜索将向量检索与关键词检索（BM25）结合，确保语义广度和关键词精度。
- **张量或多向量表示**：为了支持跨模态数据，采用张量或多向量表示已成为重要趋势。
- **元数据过滤**：基于日期、部门、类型等属性的过滤是业务场景中的刚性需求。

因此，RAG的检索层是基于向量搜索的复合系统，但必须集成全文搜索、重排序和元数据过滤等能力。

## RAG与记忆：同源不同流的检索

在智能体框架内，记忆机制的本质与RAG相同：都是根据当前需求从存储中检索相关信息。关键区别在于数据源：
- **RAG**：针对用户预先提供的现有静态或动态私有数据（如文档、数据库）。
- **记忆**：针对智能体在交互中实时生成或感知的动态数据（如对话历史、环境状态、工具执行结果）。

它们在技术基础上高度一致（例如向量检索、关键词匹配），可以视为应用于不同场景（"现有知识"与"交互记忆"）的相同检索能力。完整的智能体系统通常包括用于固有知识的RAG模块和用于交互历史的记忆模块。

## RAG应用

RAG在几个典型场景中展现了明确的价值：

1. **企业知识问答和内部搜索**
   通过将企业私有数据向量化并与LLM结合，RAG可以直接基于权威来源返回自然语言答案，而不是文档列表。在满足智能问答需求的同时，它本质上符合企业对数据安全、访问控制和合规性的要求。

2. **复杂文档理解和专业问答**
   对于合同和法规等结构复杂的文档，RAG的价值在于它能够在保持上下文完整性的同时生成准确、可验证的答案。其系统准确性很大程度上取决于文本分块和语义理解策略。

3. **动态知识融合和决策支持**
   在需要综合多个来源信息的业务场景中，RAG演变为业务决策的知识编排和推理支持系统。通过多路径召回机制，它融合来自不同系统和格式的知识，在生成阶段保持事实一致性和逻辑可控性。

## RAG的未来

RAG的演进正沿着几个清晰的路径展开：

1. **RAG作为智能体的数据基础**
   RAG与智能体之间存在架构与场景的关系。智能体要实现自主可靠的决策和执行，必须依赖准确及时的知识。RAG为它们提供了访问私有领域知识的标准化能力，是构建知识感知智能体的必然选择。

2. **高级RAG：使用LLM优化检索本身**
   下一代RAG的核心特征是充分利用LLM的推理能力来优化检索过程，例如重写查询、总结或融合结果，或实现智能路由。用LLM赋能检索的每个方面是突破当前性能瓶颈的关键。

3. **迈向上下文工程2.0**
   当前的RAG可以被视为上下文工程1.0，其核心是为单个问答任务组装静态知识上下文。即将到来的上下文工程2.0将以RAG技术为核心扩展，成为自动为智能体动态组装全面上下文的系统。该系统融合的上下文不仅来自文档，还包括交互记忆、可用工具/技能和实时环境信息。这标志着智能体开发从"手工作坊"模式转向自动化上下文工程的工业起点。

RAG的本质是为大语言模型构建专用、高效、可信的外部数据接口；其核心是检索而非生成。从解决私有数据访问的实际需求出发，其技术深度体现在针对复杂非结构化数据的检索优化上。随着其与智能体架构的深度集成和向自动化上下文工程的发展，RAG正在从提高问答质量的技术演变为构建下一代可信、可控、可扩展智能应用的核心基础设施。
